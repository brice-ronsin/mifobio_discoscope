{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExb3YxYzVhbXJzajQwZnUyMGYwY2twZG02b2N2MGlhcGFqb3E5enljcSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/9jwR2KCuAf8aIANOUr/giphy.gif\" alt=\"discoscope\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExMHRraWd5Ynh0ZXd2MDMzNHQyNjg1aDdtamYzOGdnNWJ5ZnA4dHpxZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/DlzkauL6oUw8jg8ny2/giphy.gif\" width=\"700\" alt=\"presentation\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxb8_h-QFErO"
   },
   "source": [
    "# 1.&nbsp;Insaller les dépendances de la détection d'objet TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7EOtpvlLeS0"
   },
   "source": [
    "Toutd'abord, nous allons installer l'API de dectection d'objets TensorFlow. Cela nécéssite de cloner le dépot [TensorFlow models repository](https://github.com/tensorflow/models) et d'exécuter quelques commandes d'installation. </p>\n",
    "Pour des raisons de compatibilité de version python et de raspberry, nous avonsutiliséles version 3.9.2 de python et la version 2.8.0 de TensorFlow.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\bibi\\test_atelier\\mifobio_discoscope\\tflite\\lib\\site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "# on met à jour la version de pip \n",
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/bibi/test_atelier/mifobio_discoscope\n"
     ]
    }
   ],
   "source": [
    "# on va stocker le chemin du repertoire de travail dans la variable WORK_PATH\n",
    "WORK_PATH=%pwd\n",
    "#comme nous somme sur windows le chemin stocké est de forme : D:\\folder\\files\n",
    "#pour que cela fonctionne en python le chemin doit être de la forme D:/folder/files \n",
    "# pour cela on utilise la fonction replace\n",
    "WORK_PATH=WORK_PATH.replace(\"\\\\\",\"/\")\n",
    "print(WORK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "Updating files:  19% (758/3926)\n",
      "Updating files:  20% (786/3926)\n",
      "Updating files:  21% (825/3926)\n",
      "Updating files:  22% (864/3926)\n",
      "Updating files:  23% (903/3926)\n",
      "Updating files:  24% (943/3926)\n",
      "Updating files:  25% (982/3926)\n",
      "Updating files:  26% (1021/3926)\n",
      "Updating files:  27% (1061/3926)\n",
      "Updating files:  27% (1072/3926)\n",
      "Updating files:  28% (1100/3926)\n",
      "Updating files:  29% (1139/3926)\n",
      "Updating files:  30% (1178/3926)\n",
      "Updating files:  31% (1218/3926)\n",
      "Updating files:  32% (1257/3926)\n",
      "Updating files:  33% (1296/3926)\n",
      "Updating files:  34% (1335/3926)\n",
      "Updating files:  35% (1375/3926)\n",
      "Updating files:  36% (1414/3926)\n",
      "Updating files:  37% (1453/3926)\n",
      "Updating files:  38% (1492/3926)\n",
      "Updating files:  39% (1532/3926)\n",
      "Updating files:  40% (1571/3926)\n",
      "Updating files:  41% (1610/3926)\n",
      "Updating files:  42% (1649/3926)\n",
      "Updating files:  43% (1689/3926)\n",
      "Updating files:  44% (1728/3926)\n",
      "Updating files:  44% (1740/3926)\n",
      "Updating files:  45% (1767/3926)\n",
      "Updating files:  46% (1806/3926)\n",
      "Updating files:  47% (1846/3926)\n",
      "Updating files:  48% (1885/3926)\n",
      "Updating files:  49% (1924/3926)\n",
      "Updating files:  50% (1963/3926)\n",
      "Updating files:  51% (2003/3926)\n",
      "Updating files:  52% (2042/3926)\n",
      "Updating files:  53% (2081/3926)\n",
      "Updating files:  53% (2118/3926)\n",
      "Updating files:  54% (2121/3926)\n",
      "Updating files:  55% (2160/3926)\n",
      "Updating files:  56% (2199/3926)\n",
      "Updating files:  57% (2238/3926)\n",
      "Updating files:  58% (2278/3926)\n",
      "Updating files:  59% (2317/3926)\n",
      "Updating files:  60% (2356/3926)\n",
      "Updating files:  61% (2395/3926)\n",
      "Updating files:  61% (2412/3926)\n",
      "Updating files:  62% (2435/3926)\n",
      "Updating files:  63% (2474/3926)\n",
      "Updating files:  64% (2513/3926)\n",
      "Updating files:  65% (2552/3926)\n",
      "Updating files:  66% (2592/3926)\n",
      "Updating files:  67% (2631/3926)\n",
      "Updating files:  68% (2670/3926)\n",
      "Updating files:  69% (2709/3926)\n",
      "Updating files:  70% (2749/3926)\n",
      "Updating files:  70% (2775/3926)\n",
      "Updating files:  71% (2788/3926)\n",
      "Updating files:  72% (2827/3926)\n",
      "Updating files:  73% (2866/3926)\n",
      "Updating files:  74% (2906/3926)\n",
      "Updating files:  75% (2945/3926)\n",
      "Updating files:  76% (2984/3926)\n",
      "Updating files:  77% (3024/3926)\n",
      "Updating files:  78% (3063/3926)\n",
      "Updating files:  79% (3102/3926)\n",
      "Updating files:  80% (3141/3926)\n",
      "Updating files:  81% (3181/3926)\n",
      "Updating files:  81% (3200/3926)\n",
      "Updating files:  82% (3220/3926)\n",
      "Updating files:  83% (3259/3926)\n",
      "Updating files:  84% (3298/3926)\n",
      "Updating files:  85% (3338/3926)\n",
      "Updating files:  86% (3377/3926)\n",
      "Updating files:  87% (3416/3926)\n",
      "Updating files:  88% (3455/3926)\n",
      "Updating files:  89% (3495/3926)\n",
      "Updating files:  90% (3534/3926)\n",
      "Updating files:  91% (3573/3926)\n",
      "Updating files:  92% (3612/3926)\n",
      "Updating files:  93% (3652/3926)\n",
      "Updating files:  94% (3691/3926)\n",
      "Updating files:  95% (3730/3926)\n",
      "Updating files:  96% (3769/3926)\n",
      "Updating files:  97% (3809/3926)\n",
      "Updating files:  98% (3848/3926)\n",
      "Updating files:  99% (3887/3926)\n",
      "Updating files: 100% (3926/3926)\n",
      "Updating files: 100% (3926/3926), done.\n"
     ]
    }
   ],
   "source": [
    "#on va cloner  \"tensorflow model\"  a partir du dépot GitHub dans un dossier nommer models\n",
    "!git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on telecharge le zip du protobuf 3.18.0\n",
    "# on installe les bibliothèques nécéssaires pour le téléchargement et le \"dezipage\" \n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# on telecharge le zip du protobuf 3.18.0 a l'adresse suivante que l'on nomme protoc-3.18.0-win64.zip\n",
    "urlretrieve('https://github.com/protocolbuffers/protobuf/releases/download/v3.18.0/protoc-3.18.0-win64.zip', 'protoc-3.18.0-win64.zip')\n",
    "\n",
    "#on dezippe le fichier dans un dossier appelé proto\n",
    "with zipfile.ZipFile(WORK_PATH+\"/protoc-3.18.0-win64.zip\",'r') as zip_ref:\n",
    "    zip_ref.extractall(WORK_PATH+\"/proto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ypWGYdPlLRUN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protobuff_compilé\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# on importe la biblothèque os permettant d'avoir accès à certainse fonctions de windows\n",
    "import os\n",
    "\n",
    "# on defini de nouvelles variable MODEL_PATH pour stocker le chemin du dossier d/dossier de travail/models/research\n",
    "MODELS_PATH=os.path.join(WORK_PATH,'models','research')\n",
    "#comme nous somme sur windows le chemin stocké est de forme : D:\\folder\\files\n",
    "#pour que cela fonctionne en python le chemin doit être de la forme D:/folder/files \n",
    "# pour cela on utilise la fonction replace\n",
    "MODELS_PATH=MODELS_PATH.replace('\\\\','/')\n",
    "\n",
    "# on defini de nouvelles variable PROTOC_EXE qui stocker le chemin du dossier ou est situé le proto.exe\n",
    "PROTOC_EXE=os.path.join(WORK_PATH,'proto','bin','protoc.exe')\n",
    "PROTOC_EXE=PROTOC_EXE.replace('\\\\','/')\n",
    "\n",
    "#on se positionne dans le dossier lecteur:/Dossier de travail/models/research\n",
    "os.chdir(MODELS_PATH)\n",
    "\n",
    "\n",
    "\n",
    "#on execute le fichier proto.exe sur tous les fichiers du  dossier lecteur:/Dossier de travail/models/research/object_detection/protos\n",
    "#cette fonction transforme tous les fichiers .proto en fichier python .py\n",
    "!{PROTOC_EXE} object_detection\\protos\\*.proto --python_out=.\n",
    "print(\"Protobuff_compilé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NRBnuCKjM4Bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier a été mis à jour et enregistré.\n"
     ]
    }
   ],
   "source": [
    "# Modifiez le fichier setup.py pour installer le tf-models-official du repository destiné à TF v2.8.0.\n",
    "\n",
    "import re\n",
    "\n",
    "# Remplacez le chemin ci-dessous par le chemin approprié sur votre machine locale\n",
    "input_path = os.path.join(MODELS_PATH,'object_detection/packages/tf2/setup.py')\n",
    "output_path = os.path.join(MODELS_PATH,'setup.py')\n",
    "\n",
    "# Lire le contenu du fichier\n",
    "with open(input_path, 'r') as f:\n",
    "    s = f.read()\n",
    "\n",
    "# Modifier le contenu et écrire dans un nouveau fichier\n",
    "s = re.sub('tf-models-official>=2.5.1', 'tf-models-official==2.8.0', s)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    f.write(s)\n",
    "\n",
    "print(\"Le fichier a été mis à jour et enregistré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLDnCkLLwLr6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installez l'API de détection d'objets (REMARQUE : l'exécution de ce bloc prend environ 10 minutes)\n",
    "# Il faut procéder à une correction temporaire avec PyYAML\n",
    "\n",
    "!pip install pyyaml==5.3\n",
    "!pip install {MODELS_PATH}\n",
    "# Il est nécessaire de revenir à la version TF v2.8.0 du à un bug de compatibilité avec TF v2.10 (au 10/03/22).\n",
    "!pip install tensorflow==2.8.0\n",
    "\n",
    "# Installer CUDA version 11.0 (compatible avec TF v2.8.0)\n",
    "!pip install tensorflow_io==0.23.1\n",
    "    \n",
    "#!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
    "#!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "#!wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
    "#!dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n",
    "#!apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n",
    "#!apt-get update && sudo apt-get install cuda-toolkit-11-0\n",
    "#!export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V7TrfUos-9E"
   },
   "source": [
    "Vous pouvez obtenir des avertissements ou des erreurs liés aux dépendances des paquets dans le bloc de code précédent, mais vous pouvez les ignorer pour l'instant.\n",
    "\n",
    "Testons notre installation en exécutant `model_builder_tf2_test.py` pour nous assurer que tout fonctionne comme prévu. Exécutez le bloc de code suivant et vérifiez qu'il se termine sans erreur. Si vous obtenez des erreurs, essayez de les rechercher sur Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nous allons installer quelques bibliothèques nécéssaires pour le fonctionnement \n",
    "!pip install protobuf==3.20.3\n",
    "!pip install numpy==1.23.1\n",
    "!pip install absl-py==1.4\n",
    "!pip install tensorflow-metadata==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh_HPMOqWH9z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exécutons le fichier de test Model Builder, juste pour vérifier que tout fonctionne correctement.\n",
    "\n",
    "TEST_PATH=os.path.join(MODELS_PATH,'object_detection/builders')\n",
    "TEST_PATH=TEST_PATH.replace(\"\\\\\",\"/\")\n",
    "os.chdir(TEST_PATH)\n",
    "\n",
    "!python model_builder_tf2_test.py\n",
    "print(\"finish no errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p--K1PJXEgNo"
   },
   "source": [
    "# 2 Création des Labelmap et TFRecords \n",
    "\n",
    "Nous devons créer une carte des étiquettes et convertir les images dans un format de fichier de données appelé TFRecords, qui est utilisé par TensorFlow pour l'entraînement. Nous utiliserons des scripts Python pour convertir automatiquement les données au format TFRecord. Avant de les exécuter, nous devons définir une carte des étiquettes pour nos classes.\n",
    "\n",
    "La section de code ci-dessous créera un fichier « labelmap.txt » contenant une liste de classes. Remplacez éventuellement le texte<font color='green'> droite, gauche, double </font> par vos propres classes, en ajoutant une nouvelle ligne pour chaque classe. Ensuite, cliquez sur « play » pour exécuter le code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DE_r4MKY7ln"
   },
   "outputs": [],
   "source": [
    "### Crée un fichier « labelmap.txt » contenant une liste des classes que le modèle de détection détectera.\n",
    "\n",
    "os.chdir(WORK_PATH)\n",
    "with open('labelmap.txt', 'a') as f:\n",
    "    f.write(\"droite\\n\")\n",
    "    f.write(\"gauche\\n\")\n",
    "    f.write(\"double\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pa2VYhTIT1l"
   },
   "source": [
    "Téléchargez et exécutez les scripts de conversion de données à partir [GitHub repository](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) en cliquant sur « Play » dans les trois sections de code suivantes. Ils créeront des fichiers TFRecord pour les ensembles de données d'entraînement et de validation, ainsi qu'un fichier `labelmap.pbtxt`contenant la carte des étiquettes dans un format différent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laZZE0TlEeUF"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# téléchargement des datas et des scipts de conversion\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "urlretrieve('https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_csv.py', 'create_csv.py')\n",
    "urlretrieve('https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py', 'create_tfrecord.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tdDbTmHYwu-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creation des fichiers CSV data et TFRecord\n",
    "\n",
    "# on revient dans le dossier d'origine\n",
    "os.chdir(WORK_PATH)\n",
    "\n",
    "# on lance les scripts de creation csv\n",
    "!python {WORK_PATH}/create_csv.py \n",
    "!python create_tfrecord.py --csv_input={WORK_PATH}/images/train_labels.csv --labelmap={WORK_PATH}/labelmap.txt --image_dir={WORK_PATH}/images/train --output_path={WORK_PATH}/train.tfrecord\n",
    "!python create_tfrecord.py --csv_input={WORK_PATH}/images/validation_labels.csv --labelmap={WORK_PATH}/labelmap.txt --image_dir={WORK_PATH}/images/validation --output_path={WORK_PATH}/val.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNyv_YyDXwMs"
   },
   "source": [
    "Nous allons stocker les emplacements des fichiers TFRecord et labelmap sous forme de variables afin de pouvoir les référencer plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUd2wtfrqedy"
   },
   "outputs": [],
   "source": [
    "train_record_fname =(WORK_PATH+'/train.tfrecord')\n",
    "val_record_fname =(WORK_PATH+'/val.tfrecord')\n",
    "label_map_pbtxt_fname =(WORK_PATH+'/labelmap.pbtxt')\n",
    "print(train_record_fname)\n",
    "print(val_record_fname)\n",
    "print(label_map_pbtxt_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGEUZYAMEZ6f"
   },
   "source": [
    "# 3.&nbsp;Configuration de l'entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2MAcgJ53STW"
   },
   "source": [
    "\n",
    "Nous allons maintenant configurer le modèle et l'entraînement. Pour cela nous allons spécifier quel modèle TensorFlow pré-entraîné nous voulons utiliser parmi ceux proposés dans le [TensorFlow 2 Object Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Chaque modèle est accompagné d'un fichier de configuration qui indique l'emplacement des fichiers, définit les paramètres d'entraînement (tels que le taux d'apprentissage et le nombre total d'étapes d'entraînement), et plus encore. Nous allons modifier le fichier de configuration pour notre tâche d'entraînement.\n",
    "\n",
    "La première section du code répertorie certains modèles disponibles dans le TF2 Model Zoo et définit certains noms de fichiers qui seront utilisés ultérieurement pour télécharger le modèle et le fichier de configuration. Cela facilite la gestion du modèle que vous utilisez et l'ajout ultérieur d'autres modèles à la liste.\n",
    "\n",
    "Définissez la variable « chosen_model » pour qu'elle corresponde au nom du modèle que vous souhaitez entraîner. Elle est actuellement configurée pour utiliser le modèle populaire « ssd-mobilenet-v2 ». Cliquez sur « play » dans le bloc suivant une fois que le modèle choisi a été défini.\n",
    "\n",
    "Vous ne savez pas quel modèle choisir ? Consultez [ce lien](https://ejtech.io/learn/tflite-object-detection-model-comparison) pour comparer la vitesse et la précision de chaque modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gN0EUEa3e5Un"
   },
   "outputs": [],
   "source": [
    "# chnager la variable chosen_model pour déployer un autre model variable to deploy different models available in the TF2 object detection zoo\n",
    "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
    "\n",
    "MODELS_CONFIG = {\n",
    "    'ssd-mobilenet-v2': {\n",
    "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
    "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
    "    },\n",
    "    'efficientdet-d0': {\n",
    "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
    "    },\n",
    "    'ssd-mobilenet-v2-fpnlite-320': {\n",
    "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
    "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
    "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
    "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n",
    "\n",
    "print(model_name)\n",
    "print(pretrained_checkpoint)\n",
    "print(base_pipeline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMG3EEPqPggV"
   },
   "source": [
    "Téléchargez le fichier du modèle pré-entraîné et le fichier de configuration dans la section suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va Créer un dossier « mymodel » pour stocker les poids pré-entraînés et les fichiers de configuration\n",
    "MYMODEL_PATH=os.path.join(WORK_PATH,\"models\")\n",
    "MYMODEL_PATH=MYMODEL_PATH.replace(\"\\\\\",\"/\")\n",
    "os.chdir(MYMODEL_PATH)\n",
    "%mkdir mymodel\n",
    "MYMODEL_PATH=os.path.join(MYMODEL_PATH,\"mymodel\").replace(\"\\\\\",\"/\")\n",
    "print(MYMODEL_PATH)\n",
    "\n",
    "os.chdir(MYMODEL_PATH)\n",
    "\n",
    "# on télécharge le poids du modèle pré-entraîné et on le décompresse\n",
    "import tarfile\n",
    "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
    "urlretrieve('http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint, pretrained_checkpoint)\n",
    "tar = tarfile.open(pretrained_checkpoint)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "\n",
    "# Télécharger le fichier de configuration du modèle\n",
    "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
    "urlretrieve('https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file, base_pipeline_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFAlqNrPn5y3"
   },
   "source": [
    "\n",
    "Maintenant que nous avons téléchargé notre modèle et notre fichier de configuration, nous devons modifier le fichier de configuration avec certains paramètres d'entraînement. Les variables suivantes sont utilisées pour contrôler les étapes d'entraînement :\n",
    "\n",
    "* **num_steps**: nombre total d'étapes à utiliser pour l'entraînement du modèle. Vous pouvez utiliser plus d'étapes si vous remarquez que les mesures de perte continuent de diminuer à la fin de l'entraînement. Plus il y a d'étapes, plus l'entraînement sera long. L'entraînement peut également être arrêté prématurément si la perte se stabilise avant d'atteindre le nombre d'étapes spécifié.\n",
    "* **batch_size** : nombre d'images à utiliser par étape d'entraînement. Une taille de lot plus importante permet d'entraîner un modèle en moins d'étapes, mais la taille est limitée par la mémoire GPU disponible pour l'entraînement. Avec les GPU utilisés 16 est un bon nombre pour les modèles SSD et 4 est un bon nombre pour les modèles EfficientDet.\n",
    "\n",
    "D'autres informations relatives à l'entraînement, telles que l'emplacement du fichier du modèle pré-entraîné, le fichier de configuration et le nombre total de classes, sont également attribuées à cette étape. Pour en savoir plus sur la configuration de l'entraînement avec l'API TensorFlow Object Detection,vous pouvez lire cet [article](https://neptune.ai/blog/tensorflow-object-detection-api-best-practices-to-training-evaluation-deployment). ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lYDvJN-n69v"
   },
   "outputs": [],
   "source": [
    "# definissez le nombre d'étapes et la batch size\n",
    "num_steps = 2900\n",
    "\n",
    "if chosen_model == 'efficientdet-d0':\n",
    "  batch_size = 4\n",
    "else:\n",
    "  batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on défini l'emplacement des fichiers et du \"chekpoint\" \n",
    "pipeline_fname=MYMODEL_PATH+\"/\"+base_pipeline_file\n",
    "pipeline_fname=pipeline_fname.replace('\\\\','/')\n",
    "fine_tune_checkpoint = MYMODEL_PATH+\"/\" + model_name + '/checkpoint/ckpt-0'\n",
    "fine_tune_checkpoint=fine_tune_checkpoint.replace(\"\\\\\",\"/\")\n",
    "\n",
    "print(pipeline_fname)\n",
    "print(fine_tune_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_ki9jOqxn7V"
   },
   "outputs": [],
   "source": [
    "#On défini le nombre de classes pour le fichier de configuration à partir des classes défini et on vérifie en l'écrivant dans la console.\n",
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())\n",
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
    "print('Total classes:', num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwPyaIAXxyKu"
   },
   "source": [
    "Ensuite, nous allons réécrire le fichier de configuration afin d'utiliser les paramètres d'entraînement que nous venons de spécifier. La section de code suivante remplacera automatiquement les paramètres nécessaires dans le fichier .config téléchargé et l'enregistrera sous le nom de fichier personnalisé « pipeline_file.config »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eA5ht3_yukT"
   },
   "outputs": [],
   "source": [
    "#Le code ci dessous crée un fichier de configuration personnalisé avec nos données en écrivant l'ensemble de données\n",
    "# le point de contrôle du modèle et les paramètres d'entraînement dans le fichier de pipeline_file.\n",
    "\n",
    "import re\n",
    "os.chdir(MYMODEL_PATH)\n",
    "\n",
    "print('writing custom configuration file')\n",
    "\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "with open('pipeline_file.config', 'w') as f:\n",
    "\n",
    "    # Set fine_tune_checkpoint path\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "\n",
    "    # Set tfrecord files for train and test datasets\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
    "\n",
    "    # Set label_map_path\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "\n",
    "    # Set batch_size\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               'batch_size: {}'.format(batch_size), s)\n",
    "\n",
    "    # Set training steps, num_steps\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               'num_steps: {}'.format(num_steps), s)\n",
    "\n",
    "    # Set number of classes num_classes\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               'num_classes: {}'.format(num_classes), s)\n",
    "\n",
    "    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n",
    "    s = re.sub(\n",
    "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "\n",
    "    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n",
    "    if chosen_model == 'ssd-mobilenet-v2':\n",
    "      s = re.sub('learning_rate_base: .8',\n",
    "                 'learning_rate_base: .08', s)\n",
    "\n",
    "      s = re.sub('warmup_learning_rate: 0.13333',\n",
    "                 'warmup_learning_rate: .026666', s)\n",
    "\n",
    "    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n",
    "    if chosen_model == 'efficientdet-d0':\n",
    "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
    "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
    "      s = re.sub('min_dimension', 'height', s)\n",
    "      s = re.sub('max_dimension', 'width', s)\n",
    "\n",
    "    f.write(s)\n",
    "print(MYMODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDySP7TLzdCM"
   },
   "source": [
    "Si vous êtes curieux, vous pouvez afficher le contenu du fichier de configuration ici dans le navigateur en exécutant la ligne de code ci-dessous.\n",
    "mais cette cellule n'est pas obligatoire pour le bon fonctionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEsOLOMHzBqF"
   },
   "outputs": [],
   "source": [
    "# cellule facultative pour info\n",
    "with open('pipeline_file.config', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Suivi de l'apprentissage sous Tensor Board\n",
    "\n",
    "Nous allons créer un dossier « training »  « train » et  « eval » pour stocker les points de contrôle et les données de tensorbord.\n",
    "Cela permettra de visualiser les courbes de Loss et de learning rate au fur et à mesure de l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"training\" folder for checkpoints and \"train\" forlder inside training folder for tensorboard \n",
    "# Créer un dossier « training » pour les points de contrôle et les dossiers « train » et « eval » \n",
    "# à l'intérieur du dossier training pour tensorboard. \n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(WORK_PATH)\n",
    "%mkdir training\n",
    "TRAINING=os.path.join(WORK_PATH,\"training\")\n",
    "TRAINING=TRAINING.replace('\\\\','/')\n",
    "os.chdir(TRAINING)\n",
    "%mkdir train\n",
    "%mkdir eval\n",
    "\n",
    "\n",
    "EVAL=os.path.join(TRAINING,\"eval\")\n",
    "EVAL=EVAL.replace(\"\\\\\",\"/\")\n",
    "TRAIN=os.path.join(TRAINING,\"train\")\n",
    "TRAIN=TRAIN.replace('\\\\','/')\n",
    "os.chdir(MYMODEL_PATH)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXpnXYC908Zl"
   },
   "source": [
    "> Définissons par des variables les emplacements du fichier de configuration et du répertoire de sortie du modèle afin de pouvoir les appeler lors de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Défini le chemin d'accès au fichier de configuration personnalisé et au répertoire dans lequel stocker les points de contrôle.\n",
    "MODEL_PATH_EXE=os.path.join(MODELS_PATH,\"object_detection\",\"model_main_tf2.py\")\n",
    "pipeline_file = MYMODEL_PATH+'/pipeline_file.config'\n",
    "model_dir = TRAINING+'/'\n",
    "\n",
    "print(pipeline_file)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxPj_QV43qD5"
   },
   "source": [
    ">Nous sommes prêts à entraîner notre modèle de détection d'objets ! Avant de commencer l'entraînement, chargeons une session TensorBoard pour suivre la progression de l'entraînement. Une session TensorBoard apparaîtra dans le navigateur mais vide, car nous n'avons pas encore commencé l'entraînement. Une fois l'entraînement lancé, revenez ici et cliquez sur le bouton d'actualisation pour voir les courbes de \"Loss\" et \"Learning rate\" du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les répertoires de logs\n",
    "train_log_dir = TRAIN \n",
    "val_log_dir = EVAL \n",
    "log_dir = {'train': train_log_dir, 'validation': val_log_dir} \n",
    "print(train_log_dir)\n",
    "print(val_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voir si cela est utile sinon supprimer la cellule\n",
    "import os\n",
    "\n",
    "EVAL = os.path.join(TRAINING, \"eval\")\n",
    "EVAL=EVAL.replace(\"\\\\\",\"/\")\n",
    "print(EVAL)\n",
    "\n",
    "for root, dirs, files in os.walk(EVAL):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TI9iCCxoNlAL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lance le tensor board et le dossier ou stocker les chekpoints et graph\n",
    "log_dir=TRAINING\n",
    "print(log_dir)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $log_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Netoyage de la mémoire de TeansorBoard\n",
    "\n",
    "> ⚠️ **Important :** Uniquement à lancer si vous avez déjà un graphique montrant des courbes de \"loss\" et \"d'évaluation\" <br>\n",
    "> &ensp;  &ensp;  &ensp; il faut effacer les fichiers résident en mémoire pour cela suivez les instructions suivantes :\n",
    "\n",
    "\n",
    "#### 1 lancer les deux cellules suivantes pour tuer le processus memoire de Tensorboard\n",
    "\n",
    "#### 2 lancer la troisième cellule pour supprimer tous les fichiers dans les dossier \"training\" \"eval\" et \"train\" <br>  &ensp; &ensp; contenant tous les anciens chekpoints\n",
    "\n",
    "#### 3 effacer manuelement tous les fichiers temporaires dans C:\\Users\\Nom_utilisateur\\AppData\\Local\\Temp <br>  &ensp; &ensp; ignorer ceux qui posent un problème\n",
    "\n",
    "#### 4 relancer la cellule en juste en haut de ce commentaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Chercher tous les processus et tuer celui de TensorBoard\n",
    "for proc in psutil.process_iter(attrs=['pid', 'name']):\n",
    "    if 'tensorboard' in proc.info['name'].lower():\n",
    "        print(f\"Tuer le processus TensorBoard (PID: {proc.info['pid']})\")\n",
    "        proc.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effacement de tous les fichiers chekpoints dans le dossier \"Training\" \n",
    "\n",
    "\n",
    "import os, glob\n",
    " \n",
    "dir = TRAINING\n",
    "filelist = glob.glob(os.path.join(dir, \"*\"))\n",
    "try:\n",
    "    for f in filelist:\n",
    "        os.remove(f)\n",
    "except PermissionError as e:\n",
    "    print (\"fichiers du dossier training effacés\")\n",
    "\n",
    "# Effacement de tous les fichiers chekpoints dans le dossier \"train\" \n",
    "dir1 = train_log_dir\n",
    "filelist = glob.glob(os.path.join(dir1, \"*\"))\n",
    "try:\n",
    "    for f in filelist:\n",
    "        os.remove(f)\n",
    "except PermissionError as e:\n",
    "    print (\"fichiers du dossier train effacés\")\n",
    "\n",
    "# Effacement de tous les fichiers chekpoints dans le dossier \"eval\" \n",
    "dir2 = val_log_dir\n",
    "filelist = glob.glob(os.path.join(dir2, \"*\"))\n",
    "try:\n",
    "    for f in filelist:\n",
    "        os.remove(f)\n",
    "except PermissionError as e:\n",
    "    print (\"fichiers du dossier train effacés\")\n",
    "\n",
    "print(\"tous les fichiers sont effacés\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cuQpPJL2pUq"
   },
   "source": [
    "### 3.3 Lancement de l'apprentissage\n",
    "\n",
    "L'entrainement sur notre jeux de données est effectuée à l'aide du script « model_main_tf2.py » de l'API TF Object Detection. Cet entrainement prendra entre 2 et 6 heures, selon le modèle, l'utilisation ou non des GPU, la taille du lot et le nombre d'étapes. Nous avons déjà défini tous les paramètres et arguments utilisés par « model_main_tf2.py » dans les sections précédentes. Cependant, le script « model_main_tf2.py » n'affiche que les courbes de train, nous avons donc modifié ce code afin qu'il montre dans TensorBoard les courbes de train et de validation. nous allons dans la première cellule remplacer le fichier model_main_tf2.py fournit par TF Object Detection par le notre situé dans le dossier a_copier\n",
    "\n",
    "\n",
    "\n",
    "> *Remarque : le programme met quelques minutes à afficher les messages d'entraînement, car il n'affiche les journaux qu'une fois tous les 100 pas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source = os.path.join(WORK_PATH,\"a_copier\",\"model_main_tf2.py\")\n",
    "source=source.replace('\\\\','/')\n",
    "\n",
    "destination = os.path.join(MODELS_PATH,\"object_detection\")\n",
    "destination=destination.replace('\\\\','/')\n",
    "\n",
    "shutil.copy(source, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_probability==0.24.0\n",
    "!pip install tensorstore==0.1.66\n",
    "!pip install absl-py==1.2\n",
    "!pip install tensorflow-metadata==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on defini une variable Model_Path_exe qui est le fichier nécéssaire pour lancer l'apprentissage \n",
    "MODEL_PATH_EXE=os.path.join(MODELS_PATH,\"object_detection\",\"model_main_tf2.py\")\n",
    "MODEL_PATH_EXE=MODEL_PATH_EXE.replace(\"\\\\\",\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques print pour verifier les chemins utilisé pour la configuration\n",
    "print(\"MODEL_PATH_EXE:\", MODEL_PATH_EXE)\n",
    "print(\"pipeline_file:\", pipeline_file)\n",
    "print(\"model_dir:\", model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code de lancement de l'apprentissage avec affichage dynamique des étapes dans la console (adapté à un notebook Jupyter)\n",
    "\n",
    "import subprocess         # Pour exécuter des commandes système (lancement du script d'entraînement)\n",
    "import sys                # Pour obtenir le chemin de l'interpréteur Python courant\n",
    "import threading          # Pour gérer l'exécution parallèle (lecture des flux en temps réel)\n",
    "import queue              # Pour stocker les lignes de sortie à lire sans blocage\n",
    "from IPython.display import display, clear_output  # Pour afficher dynamiquement les logs dans un notebook\n",
    "from collections import deque  # File optimisée pour stocker un nombre limité de lignes (FIFO)\n",
    "\n",
    "# Affichage du fichier de configuration utilisé (variable supposée définie en amont)\n",
    "print(\"Lecture du fichier de configuration du pipeline : \", pipeline_file)\n",
    "\n",
    "# Fonction utilitaire pour lire la sortie d’un flux (stdout ou stderr) dans un thread séparé\n",
    "def enqueue_output(out, q):\n",
    "    # Itère sur chaque ligne du flux et l’ajoute à la queue jusqu’à la fin du flux\n",
    "    for line in iter(out.readline, ''):\n",
    "        q.put(line)\n",
    "    out.close()\n",
    "\n",
    "# Fonction principale pour lancer le processus d'entraînement et afficher les logs en temps réel\n",
    "def run_process():\n",
    "    # Construction de la commande à exécuter (le script Python avec ses arguments)\n",
    "    command = [\n",
    "        sys.executable,  # Utilise l'interpréteur Python actuel\n",
    "        '{}'.format(MODEL_PATH_EXE),  # Chemin vers le script Python d'entraînement\n",
    "        '--pipeline_config_path={}'.format(pipeline_file),  # Chemin du fichier de config du pipeline\n",
    "        '--model_dir={}'.format(model_dir),  # Répertoire de sortie du modèle\n",
    "        '--alsologtostderr',  # Redirige les logs également vers stderr\n",
    "        '--num_train_steps={}'.format(num_steps),  # Nombre d'étapes d'entraînement\n",
    "        '--sample_1_of_n_eval_examples=1'  # Utilise tous les exemples pour l'éval (pas d’échantillonnage)\n",
    "        # D'autres options peuvent être ajoutées ici, comme les répertoires de logs\n",
    "    ]\n",
    "\n",
    "    # Lancement du processus avec capture de stdout et stderr\n",
    "    process = subprocess.Popen(\n",
    "        command,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,             # Assure que la sortie est en texte (str) et non binaire\n",
    "        bufsize=1,             # Lecture ligne par ligne\n",
    "        universal_newlines=True\n",
    "    )\n",
    "\n",
    "    # Création d’une file thread-safe pour collecter les lignes de sortie\n",
    "    q = queue.Queue()\n",
    "\n",
    "    # Création de deux threads pour lire simultanément stdout et stderr\n",
    "    t1 = threading.Thread(target=enqueue_output, args=(process.stdout, q))\n",
    "    t2 = threading.Thread(target=enqueue_output, args=(process.stderr, q))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Stocke les dernières lignes de logs (jusqu'à 1000) pour les afficher dynamiquement\n",
    "    log_lines = deque(maxlen=1000)\n",
    "    seen_lines = set()  # Ensemble pour éviter d’afficher plusieurs fois la même ligne\n",
    "\n",
    "    # Boucle principale de lecture et d’affichage\n",
    "    while True:\n",
    "        try:\n",
    "            # Essaye de lire une ligne de la file sans attendre\n",
    "            line = q.get_nowait()\n",
    "        except queue.Empty:\n",
    "            pass  # Si rien n’est lu, ne fait rien pour cette itération\n",
    "        else:\n",
    "            # Si la ligne n’a pas encore été affichée, on l’ajoute\n",
    "            if line not in seen_lines:\n",
    "                seen_lines.add(line)\n",
    "                log_lines.append(line.strip())  # Nettoie les espaces inutiles\n",
    "                clear_output(wait=True)        # Efface la sortie précédente (pour un affichage en direct)\n",
    "                print(\"\\n\".join(log_lines))    # Affiche les dernières lignes\n",
    "\n",
    "        # Si le processus est terminé, on sort de la boucle\n",
    "        if process.poll() is not None:\n",
    "            break\n",
    "\n",
    "    # On attend que les deux threads aient fini leur lecture\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "# Appel de la fonction avec gestion des erreurs éventuelles\n",
    "try:\n",
    "    run_process()\n",
    "except Exception as e:\n",
    "    print(\"Erreur lors de l'exécution de la commande : \", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPg8oMnQDYKl"
   },
   "source": [
    "# 4.&nbsp;Convertion du modèle en TensorFlowLite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spQXdq8Y63pj"
   },
   "source": [
    "Très bien !L'apprentissage est dini et notre modèle est prêt à être utilisé pour détecter des objets. Tout d'abord, nous devons exporter le graphe du modèle (un fichier contenant des informations sur l'architecture et les poids) dans un format compatible avec TensorFlow Lite. Pour ce faire, nous utiliserons le script  `export_tflite_graph_tf2.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on revient au dossier de travail \n",
    "os.chdir(WORK_PATH)\n",
    "# on crée un dossier mon_model ou on sauvegardera le graph\n",
    "!mkdir mon_model\n",
    "CONVERT=os.path.join(WORK_PATH,'models','research','object_detection','export_tflite_graph_tf2.py')\n",
    "CONVERT=CONVERT.replace(\"\\\\\",\"/\")\n",
    "output_directory = os.path.join(WORK_PATH,'mon_model')\n",
    "output_directory=output_directory.replace('\\\\','/')\n",
    "\n",
    "\n",
    "# Chemin d'accès au répertoire de formation (le script de conversion choisit automatiquement le dernier checkpoint)\n",
    "last_model_path = (TRAINING)\n",
    "\n",
    "\n",
    "\n",
    "try :\n",
    "    !python {CONVERT} \\\n",
    "    --trained_checkpoint_dir {last_model_path} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_file}\n",
    "except Exception as e:\n",
    "    print(\"Erreur lors de la lecture du fichier : \", e)\n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ensuite, nous allons prendre le graphique exporté et utiliser le module TFLiteConverter pour le convertir au format .tflite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsE_uVjlsz3u"
   },
   "outputs": [],
   "source": [
    "# Convertir le fichier graph exporte en fichier TFLite model\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(output_directory+'/saved_model')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(output_directory+'/detect.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDQrtQhvC3oG"
   },
   "source": [
    "# 5.&nbsp;Test du model TensorFlow Lite et calcul du mAP (mean average precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtSmUZcxIAvt"
   },
   "source": [
    "Nous avons entraîné notre modèle et l'avons converti au format TFLite. Mais quelle est sa performance réelle en matière de détection d'objets dans les images ? C'est là que les images que nous avons mises de côté dans le dossier **test** entrent en jeu. Le modèle n'a jamais vu aucune image de test pendant l'entraînement, donc sa performance sur ces images devrait être représentative de sa performance sur de nouvelles images provenant d'une vérité terrain.\n",
    "\n",
    "### 5.1 Images de test d'inférence\n",
    "Le code suivant définit une fonction permettant d'effectuer une inférence sur les images du dossier test (L'inférence en deep learning vise à réaliser des prédictions efficaces à partir d'un modèle d'apprentissage entraîné). Il charge les images, charge le modèle et la carte d'étiquettes, exécute le modèle sur chaque image et affiche le résultat. Il enregistre également, de manière facultative, les résultats de la détection sous forme de fichiers texte afin que nous puissions les utiliser pour calculer le  mAP (Mean Average Precision ou  précision moyenne) du modèle.\n",
    "\n",
    "\n",
    "Ce code est basé sur le script [TFLite_detection_image.py](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py) du GitHub TensorFlow Lite Object Detection sur GitHub ; n'hésitez pas à l'utiliser comme point de départ pour vos propre applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4WtI8i5K96w"
   },
   "outputs": [],
   "source": [
    "# Script to run custom TFLite model on test images to detect objects\n",
    "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "### Define function for inferencing with TFLite model and displaying results\n",
    "\n",
    "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
    "\n",
    "  # Grab filenames of all images in test folder\n",
    "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
    "\n",
    "  # Load the label map into memory\n",
    "  with open(lblpath, 'r') as f:\n",
    "      labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "  # Load the Tensorflow Lite model into memory\n",
    "  interpreter = Interpreter(model_path=modelpath)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Get model details\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "  height = input_details[0]['shape'][1]\n",
    "  width = input_details[0]['shape'][2]\n",
    "\n",
    "  float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "  input_mean = 127.5\n",
    "  input_std = 127.5\n",
    "\n",
    "  # Randomly select test images\n",
    "  images_to_test = random.sample(images, num_test_images)\n",
    "\n",
    "  # Loop over every image and perform detection\n",
    "  for image_path in images_to_test:\n",
    "\n",
    "      # Load image and resize to expected shape [1xHxWx3]\n",
    "      image = cv2.imread(image_path)\n",
    "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      imH, imW, _ = image.shape\n",
    "      image_resized = cv2.resize(image_rgb, (width, height))\n",
    "      input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "      if float_input:\n",
    "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "      # Perform the actual detection by running the model with the image as input\n",
    "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Retrieve detection results\n",
    "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
    "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
    "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
    "\n",
    "      detections = []\n",
    "\n",
    "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "      for i in range(len(scores)):\n",
    "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "\n",
    "              # Get bounding box coordinates and draw box\n",
    "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "\n",
    "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "              # Draw label\n",
    "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
    "\n",
    "\n",
    "      # All the results have been drawn on the image, now display the image\n",
    "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(12,16))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "      # Save detection results in .txt files (for calculating mAP)\n",
    "      elif txt_only == True:\n",
    "\n",
    "        # Get filenames and paths\n",
    "        image_fn = os.path.basename(image_path)\n",
    "        base_fn, ext = os.path.splitext(image_fn)\n",
    "        txt_result_fn = base_fn +'.txt'\n",
    "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
    "\n",
    "        # Write results to text file\n",
    "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
    "        with open(txt_savepath,'w') as f:\n",
    "            for detection in detections:\n",
    "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CJI4A0f_zqz"
   },
   "source": [
    "Le bloc suivant de code définit les chemins d'accès aux images et aux modèles de test, puis exécute la le modèle. Si vous souhaitez utiliser plus de 10 images, modifiez la variable `images_to_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t8CMarqBqP9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up variables for running user's model\n",
    "# Path to test images folder\n",
    "PATH_TO_IMAGES=(os.path.join(WORK_PATH,\"images\",\"test\").replace(\"\\\\\",\"/\"))\n",
    "# Path to .tflite model file\n",
    "PATH_TO_MODEL=(os.path.join(output_directory,\"detect.tflite\").replace(\"\\\\\",\"/\"))\n",
    "# Path to labelmap.txt file\n",
    "PATH_TO_LABELS=(os.path.join(WORK_PATH,'labelmap.txt').replace(\"\\\\\",\"/\"))\n",
    "#Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "min_conf_threshold=0.7\n",
    "# Number of images to run detection on\n",
    "images_to_test = 24 \n",
    "\n",
    "# Run inferencing function!\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_ckqeWqBF0P"
   },
   "source": [
    "### 5.2 Calcul du mAP (mean Average Precision)\n",
    "Nous visualisons désormais les performances de notre modèle sur les images de test, mais comment pouvons-nous mesurer quantitativement sa précision ?\n",
    "\n",
    "Une méthode courante pour mesurer la précision d'un modèle de détection d'objets est la « précision moyenne » (mAP). En gros, plus le score mAP est élevé, plus votre modèle est performant pour détecter des objets dans des images. Pour en savoir plus sur la mAP, consultez cet [article de Roboflow](https://blog.roboflow.com/mean-average-precision/).\n",
    "\n",
    "Nous utiliserons l'outil de calcul mAP disponible à l'adresse https://github.com/Cartucho/mAP pour déterminer le score mAP de notre modèle. Tout d'abord, nous devons cloner le GitHub et supprimer les exemples de données existants. Nous téléchargerons également un script permettant d'interagir avec le calculateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlWarXEZDUqS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(WORK_PATH)\n",
    "!git clone https://github.com/Cartucho/mAP\n",
    "os.chdir(WORK_PATH+'\\mAP')\n",
    "mAP=os.path.join(WORK_PATH,'mAP')\n",
    "mAP=mAP.replace(\"\\\\\",\"/\")\n",
    "files = glob.glob('input/detection-results/*') \n",
    "for f in files: \n",
    "    os.remove(f)\n",
    "files = glob.glob('input/ground-truth/*') \n",
    "for f in files: \n",
    "    os.remove(f)\n",
    "files = glob.glob('input/images-optional/*') \n",
    "for f in files: \n",
    "    os.remove(f)\n",
    "!curl -O https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn22nGGqH5T6"
   },
   "source": [
    "Dans la cellule suivante nous copierons les images et les données d'annotation du dossier **test** vers les dossiers appropriés pour le calcul de la mAP. Celles-ci seront utilisées comme « données de référence » en comparaison des résultats de notre modèle .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "src = os.path.join(WORK_PATH,'images','test').replace('\\\\','/')\n",
    "dest=os.path.join(mAP,'input','images-optional').replace('\\\\','/')\n",
    "dest2=os.path.join(mAP,'input','ground-truth').replace('\\\\','/')\n",
    "for file in (glob.glob(src+\"/*\")):\n",
    "    shutil.copy2(file, dest)\n",
    "\n",
    "old_file=glob.glob(dest2+'/*')\n",
    "for all_file in old_file:\n",
    "    os.remove(all_file)\n",
    "\n",
    "for file_xml in (glob.glob(dest+\"/*.xml*\")):\n",
    "    file_xml=(file_xml).replace('\\\\','/')\n",
    "    shutil.move(file_xml,dest2)\n",
    "   \n",
    "print (\"all done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6aro817DGzx"
   },
   "source": [
    "L'outil de calcul attend des données d'annotation dans un format différent le format de fichier.xml que nous utilisons. Heureusement, il fournit un script simple, `convert_gt_xml.py`, pour la convertion au format .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdjtOUDnK2AA"
   },
   "outputs": [],
   "source": [
    "!python {mAP}/scripts/extra/convert_gt_xml.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnIUacAlLP0B"
   },
   "source": [
    "Nous avons configuré les données de référence, et nous avons maintenant besoin des résultats de détection réels de notre modèle. Les résultats de détection seront comparés aux données de référence afin de calculer la précision du modèle en mAP.\n",
    "\n",
    "La fonction d'inférence que nous avons définie à l'étape 5.1 est utilisée pour générer des données de détection pour toutes les images du dossier **test**. Nous l'utiliserons de la même manière que précédemment, sauf que cette fois-ci, nous lui demanderons d'enregistrer les résultats de détection dans le dossier `detection-results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables for running user's model\n",
    "# Path to test images folder\n",
    "PATH_TO_IMAGES=(os.path.join(WORK_PATH,\"images\",\"test\").replace(\"\\\\\",\"/\"))\n",
    "# Path to .tflite model file\n",
    "PATH_TO_MODEL=(os.path.join(output_directory,\"detect.tflite\").replace(\"\\\\\",\"/\"))\n",
    "# Path to labelmap.txt file\n",
    "PATH_TO_LABELS=(os.path.join(WORK_PATH,'labelmap.txt').replace(\"\\\\\",\"/\"))\n",
    "#Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "min_conf_threshold=0.7\n",
    "# Number of images to run detection on\n",
    "images_to_test = 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szzHFAhsMNFF"
   },
   "outputs": [],
   "source": [
    "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
    "# Path to test images folder\n",
    "PATH_TO_IMAGES=(os.path.join(WORK_PATH,\"images\",\"test\").replace(\"\\\\\",\"/\"))\n",
    "# Path to .tflite model file\n",
    "PATH_TO_MODEL=(os.path.join(output_directory,\"detect.tflite\").replace(\"\\\\\",\"/\"))\n",
    "# Path to labelmap.txt file\n",
    "PATH_TO_LABELS=(os.path.join(WORK_PATH,'labelmap.txt').replace(\"\\\\\",\"/\"))\n",
    "# Folder to save detection results in\n",
    "PATH_TO_RESULTS=(os.path.join(WORK_PATH,'mAP','input','detection-results').replace(\"\\\\\",\"/\"))\n",
    "min_conf_threshold=0.7   # Confidence threshold\n",
    "\n",
    "# Use all the images in the test folder\n",
    "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
    "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
    "\n",
    "# Tell function to just save results and not display images\n",
    "txt_only = True\n",
    "\n",
    "# Run inferencing function!\n",
    "print('Starting inference on %d images...' % images_to_test)\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
    "print('Finished inferencing!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_QRnTqNPX4z"
   },
   "source": [
    "Calculons enfin le mAP ! Pour rapporter le mAP nous utiliserons la métrique COCO pour mAP @ 0,50:0,95. Concrètement, cela signifie que le mAP est calculé à plusieurs seuils IoU compris entre 0,50 et 0,95, puis que le résultat de chaque seuil est moyenné pour obtenir un score mAP final. Pour en savoir plus, [cliquez ici !](https://blog.roboflow.com/mean-average-precision/)\n",
    "\n",
    "Nous utiliserons un scipt déjà pret pour exécuter l'outil de calcul à chaque seuil IoU, faire la moyenne des résultats et communiquer le score de précision final. Il communique le mAP pour chaque classe et le mAP global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DkjpIBARTQ7"
   },
   "outputs": [],
   "source": [
    "os.chdir(mAP)\n",
    "!python calculate_map_cartucho.py --labels={WORK_PATH}/labelmap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9HPoOBVKvxU"
   },
   "source": [
    "Le score indiqué à la fin correspond au score mAP global de votre modèle. Idéalement, il devrait être supérieur à 50 % (0,50). Si ce n'est pas le cas, vous pouvez améliorer la précision de votre modèle en ajoutant davantage d'images à votre ensemble de données. Vous pouvez si vous le souhaitez regarder la [vidéo de Edje Electronics](https://www.youtube.com/watch?v=v0ssiOY6cfg) pour capturer de bonnes images d'entraînement et améliorer la précision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i40ve0SCLaE"
   },
   "source": [
    "# 6.&nbsp;Déploiement du modèle TensorFlow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phT8vvzriqQp"
   },
   "source": [
    "Maintenant que votre modèle personnalisé a été entraîné et converti au format TFLite, il est prêt à être téléchargé et déployé dans une application ! Cette section explique comment télécharger et le déployer sur le Raspberry Pi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq3L2IoP4VHp"
   },
   "source": [
    "## 6.1. Téléchargement du modèle TFLite \n",
    "\n",
    "Exécutez les deux cellules suivantes pour copier les fichiers labelmap dans le dossier du modèle, les compresser dans un dossier zip, puis les télécharger. Le dossier zip contient le modèle `detect.tflite` et les fichiers labelmap `labelmap.txt` nécessaires pour exécuter le modèle sur votre raspberry PI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "src1 = os.path.join(WORK_PATH,'labelmap.txt').replace('\\\\','/')\n",
    "src2= os.path.join(WORK_PATH,'labelmap.pbtxt').replace('\\\\','/')\n",
    "src3= os.path.join(WORK_PATH,'models','mymodel','pipeline_file.config').replace('\\\\','/')\n",
    "\n",
    "\n",
    "dest_final=output_directory.replace('\\\\','/')\n",
    "\n",
    "shutil.copy2(src1, dest_final)\n",
    "shutil.copy2(src2, dest_final)\n",
    "shutil.copy2(src3, dest_final)\n",
    "\n",
    "os.chdir(WORK_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive=shutil.make_archive('mon_model','zip',output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Kb3ZBsMq95l"
   },
   "source": [
    "#### a faire ####\n",
    "The `custom_model_lite.zip` file containing the model will download into your Downloads folder. It's ready to be deployed on your device!\n",
    "\n",
    "Le fichier custom_model_lite.zip contenant le modèle sera téléchargé dans votre dossier Téléchargements. Il est prêt à être déployé sur votre appareil !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSJ2wgGCixy2"
   },
   "source": [
    "## 6.2. Déploiement du modèle\n",
    "TensorFlow Lite models can run on a wide variety of hardware, including PCs, embedded systems, and phones. This section provides instructions showing how to deploy your TFLite model on various devices.\n",
    "\n",
    "### 6.2.1. Deploy on Raspberry Pi\n",
    "TFLite models are great for running on the Raspberry Pi, because they require less processing power than regular TensorFlow vision models. The Pi can run TFLite models in near real-time.\n",
    "\n",
    "To run your new model on the Raspberry Pi, you'll have to install TensorFlow Lite and prepare a Python environment for your application. I provide step-by-step instructions on how to set up TFLite on the Pi in my video, [How To Run TensorFlow Lite on Raspberry Pi for Object Detection](https://youtu.be/aimSGOAUI8Y).\n",
    "\n",
    "[![Link to my YouTube video!](https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/doc/YouTube_video1.JPG)](https://www.youtube.com/watch?v=aimSGOAUI8Y)\n",
    "\n",
    "Once you've completed all the steps in the video, move the `custom_model_lite.zip` file downloaded from this Colab session over to your Raspberry Pi into the `~/tflite1` folder. Move into the folder and unzip it by issuing:\n",
    "\n",
    "```\n",
    "cd ~/tflite1\n",
    "unzip custom_model_lite.zip\n",
    "```\n",
    "\n",
    "Then, run the image, video, or webcam TFLite detection program with the `--modeldir=fine_tuned_model_lite` argument. For example, to run the webcam detection program, issue:\n",
    "\n",
    "```\n",
    "python TFLite_detection_webcam.py --modeldir=custom_model_lite\n",
    "```\n",
    "\n",
    "A window will appear showing a live feed from your webcam with boxes drawn around detected objects in each frame.\n",
    "\n",
    "### 6.2.2. Deploy on Windows, Linux, or macOS\n",
    "Follow the instructions linked below to quickly set up your Windows, Linux, or macOS computer to run TFLite models. It only takes a few minutes! Running a model on your PC is good for quickly testing your model with a webcam. However, keep in mind that the TFLite Runtime is optimized for lower-power processors, and it won't utilize the full capability of your PC's processor.\n",
    "\n",
    "Here are links to the deployment guides for Windows, Linux, and macOS:\n",
    "* [How to Run TensorFlow Lite Models on Windows](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/deploy_guides/Windows_TFLite_Guide.md)\n",
    "* *link to Linux guide to be added (but really it's the same as Raspberry Pi)*\n",
    "* [How to Run TensorFlow Lite Models on macOS](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/deploy_guides/MacOS_TFLite_Guide.md)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTyqlXFTJ0Uv"
   },
   "source": [
    "## 9.1. Quantize model\n",
    "We'll use the \"TFLiteConverter\" module to perform [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) on the model. To quantize the model, we need to provide a representative dataset, which is a set of images that represent what the model will see when deployed in the field. First, we'll create a list of images to include in the representative dataset (we'll just use the images in the `train` folder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSNZtfj_k3NP"
   },
   "outputs": [],
   "source": [
    "# Get list of all images in train directory\n",
    "image_path = image_path = (os.path.join(WORK_PATH,'images','test').replace(\"\\\\\",\"/\"))\n",
    "\n",
    "jpg_file_list = glob.glob(image_path + '/*.jpg')\n",
    "#JPG_file_list = glob.glob(image_path + '/*.JPG')\n",
    "#png_file_list = glob.glob(image_path + '/*.png')\n",
    "#bmp_file_list = glob.glob(image_path + '/*.bmp')\n",
    "#quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list\n",
    "quant_image_list = jpg_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqbH1VlEgiuy"
   },
   "source": [
    "Next, we'll define a function to yield images from our representative dataset. Refer to [TensorFlow's sample quantization code](https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb#scrollTo=kRDabW_u1wnv) to get a better understanding of what this is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORzx0XRErSLV"
   },
   "outputs": [],
   "source": [
    "# A generator that provides a representative dataset\n",
    "# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n",
    "\n",
    "# First, get input details for model so we know how to preprocess images\n",
    "interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "import random\n",
    "\n",
    "def representative_data_gen():\n",
    "  dataset_list = quant_image_list\n",
    "  quant_num = 300\n",
    "  for i in range(quant_num):\n",
    "    pick_me = random.choice(dataset_list)\n",
    "    image = tf.io.read_file(pick_me)\n",
    "\n",
    "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
    "      image = tf.io.decode_jpeg(image, channels=3)\n",
    "    elif pick_me.endswith('.png'):\n",
    "      image = tf.io.decode_png(image, channels=3)\n",
    "    elif pick_me.endswith('.bmp'):\n",
    "      image = tf.io.decode_bmp(image, channels=3)\n",
    "\n",
    "    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqtu98mzebEj"
   },
   "source": [
    "Finally, we'll initialize the TFLiteConverter module, point it at the TFLite graph we generated in Step 6, and provide it with the representative dataset generator function we created in the previous code block. We'll configure the converter to quantize the model's weight values to INT8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox0bGDWds_Ce"
   },
   "outputs": [],
   "source": [
    "# Initialize converter module\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(output_directory+'/saved_model')\n",
    "\n",
    "# This enables quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This sets the representative dataset for quantization\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "# These set the input tensors to uint8 and output tensors to float32\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(output_directory+'/detect_quant.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYVVlv5QUUZF"
   },
   "source": [
    "## 9.2. Test quantized model\n",
    "The model has been quantized and exported as `detect_quant.tflite`. Let's test it out! We'll re-use the function from Section 7 for running the model on test images and display the results, except this time we'll point it at the quantized model.\n",
    "\n",
    "Click Play on the code block below to test the `detect_quant.tflite` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OoirJuOtdOG"
   },
   "outputs": [],
   "source": [
    "# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n",
    "\n",
    "PATH_TO_MODEL=output_directory+'/detect_quant.tflite'   #Path to .tflite model file\n",
    "\n",
    "min_conf_threshold=0.7   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "images_to_test = 10   #Number of images to run detection on\n",
    "\n",
    "# Run inferencing function!\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKo7ZtfOyoxG"
   },
   "source": [
    "If your quantized model isn't performing very well, try using my TensorFlow Lite 1 notebook *(link to be added)* to train a SSD-MobileNet model with your dataset. In my experience, the `ssd-mobilenet-v2-quantized` model from the [TF1 Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) has the best quantized performance out of any other TensorFlow Lite model.\n",
    "\n",
    "TFLite models created with TensorFlow 1 are still compatible with the TensorFlow Lite 2 runtime, so your TFLite 1 model will still work with my [TensorFlow setup guide for the Raspberry Pi](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWdVxs6LUjbR"
   },
   "source": [
    "## 9.3 Calculate quantized model mAP\n",
    "\n",
    "Let's calculate the quantize model's mAP using the calculator tool we set up in Step 7.2. We just need to perform inference with our quantized model (`detect_quant.tflite`) to get a new set of detection results.\n",
    "\n",
    "Run the following block to run inference on the test images and save the detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMaumV-11Et0"
   },
   "outputs": [],
   "source": [
    "# Need to remove existing detection results first\n",
    "!rm /content/mAP/input/detection-results/*\n",
    "\n",
    "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
    "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
    "PATH_TO_MODEL='/content/custom_model_lite/detect_quant.tflite'   # Path to quantized .tflite model file\n",
    "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
    "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
    "min_conf_threshold=0.1   # Confidence threshold\n",
    "\n",
    "# Use all the images in the test folder\n",
    "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
    "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
    "\n",
    "# Tell function to just save results and not display images\n",
    "txt_only = True\n",
    "\n",
    "# Run inferencing function!\n",
    "print('Starting inference on %d images...' % images_to_test)\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
    "print('Finished inferencing!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgcmdLQf1Et1"
   },
   "source": [
    "Now we can run the mAP calculation script to determine our quantized model's mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIRNp0Af1Et1"
   },
   "outputs": [],
   "source": [
    "cd /content/mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TDgMBw_1Et1"
   },
   "outputs": [],
   "source": [
    "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4VAvZo8qE4u5",
    "sxb8_h-QFErO",
    "eydREUsMGUUR",
    "eGEUZYAMEZ6f",
    "-19zML6oEO7l",
    "kPg8oMnQDYKl",
    "RDQrtQhvC3oG",
    "5i40ve0SCLaE",
    "WoptFnAhCSrR",
    "5VI_Gh5dCd7w"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tflite",
   "language": "python",
   "name": "tflite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
